{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPpnpbEzYzejUJjneJbidT8",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/tgmeiner/exploring-generative/blob/main/test_article_summary.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Create a tool that summarizes the text in a given website\n",
        "\n",
        "\n",
        "*   scrap the text from the site\n",
        "*   summarize the text using a pre-existing transformer model (e.g. BART Large CNN - based on news article summary pairs)\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "yCGX42ISTuOD"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eTz_hXl3ToXi",
        "outputId": "cbb4d9f5-c9bf-4503-ec3e-fdb05aa2072d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting feedparser\n",
            "  Downloading feedparser-6.0.10-py3-none-any.whl (81 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m81.1/81.1 KB\u001b[0m \u001b[31m1.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting sgmllib3k\n",
            "  Downloading sgmllib3k-1.0.0.tar.gz (5.8 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Building wheels for collected packages: sgmllib3k\n",
            "  Building wheel for sgmllib3k (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for sgmllib3k: filename=sgmllib3k-1.0.0-py3-none-any.whl size=6066 sha256=937a22249a110346d48fe6ac4bc3b6ee763be03e6d98ef7b76da445edb2090a7\n",
            "  Stored in directory: /root/.cache/pip/wheels/83/63/2f/117884c3b19d46b64d3d61690333aa80c88dc14050e269c546\n",
            "Successfully built sgmllib3k\n",
            "Installing collected packages: sgmllib3k, feedparser\n",
            "Successfully installed feedparser-6.0.10 sgmllib3k-1.0.0\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting trafilatura\n",
            "  Downloading trafilatura-1.4.0-py3-none-any.whl (1.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m9.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting justext>=3.0.0\n",
            "  Downloading jusText-3.0.0-py2.py3-none-any.whl (837 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m837.8/837.8 KB\u001b[0m \u001b[31m31.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting urllib3<2,>=1.26\n",
            "  Downloading urllib3-1.26.14-py2.py3-none-any.whl (140 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m140.6/140.6 KB\u001b[0m \u001b[31m7.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: certifi in /usr/local/lib/python3.8/dist-packages (from trafilatura) (2022.12.7)\n",
            "Requirement already satisfied: lxml>=4.6.4 in /usr/local/lib/python3.8/dist-packages (from trafilatura) (4.9.2)\n",
            "Collecting htmldate>=1.3.2\n",
            "  Downloading htmldate-1.4.1-py3-none-any.whl (33 kB)\n",
            "Collecting courlan>=0.8.3\n",
            "  Downloading courlan-0.8.3-py3-none-any.whl (34 kB)\n",
            "Requirement already satisfied: charset-normalizer>=2.1.1 in /usr/local/lib/python3.8/dist-packages (from trafilatura) (2.1.1)\n",
            "Collecting tld>=0.12.6\n",
            "  Downloading tld-0.12.6-py38-none-any.whl (412 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m412.2/412.2 KB\u001b[0m \u001b[31m12.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: langcodes>=3.3.0 in /usr/local/lib/python3.8/dist-packages (from courlan>=0.8.3->trafilatura) (3.3.0)\n",
            "Collecting dateparser>=1.1.2\n",
            "  Downloading dateparser-1.1.6-py2.py3-none-any.whl (293 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m293.3/293.3 KB\u001b[0m \u001b[31m11.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.8/dist-packages (from htmldate>=1.3.2->trafilatura) (2.8.2)\n",
            "Collecting charset-normalizer>=2.1.1\n",
            "  Downloading charset_normalizer-3.0.1-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (195 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m195.4/195.4 KB\u001b[0m \u001b[31m6.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: regex!=2019.02.19,!=2021.8.27 in /usr/local/lib/python3.8/dist-packages (from dateparser>=1.1.2->htmldate>=1.3.2->trafilatura) (2022.6.2)\n",
            "Requirement already satisfied: tzlocal in /usr/local/lib/python3.8/dist-packages (from dateparser>=1.1.2->htmldate>=1.3.2->trafilatura) (1.5.1)\n",
            "Requirement already satisfied: pytz in /usr/local/lib/python3.8/dist-packages (from dateparser>=1.1.2->htmldate>=1.3.2->trafilatura) (2022.7)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.8/dist-packages (from python-dateutil>=2.8.2->htmldate>=1.3.2->trafilatura) (1.15.0)\n",
            "Installing collected packages: charset-normalizer, urllib3, tld, justext, dateparser, courlan, htmldate, trafilatura\n",
            "  Attempting uninstall: charset-normalizer\n",
            "    Found existing installation: charset-normalizer 2.1.1\n",
            "    Uninstalling charset-normalizer-2.1.1:\n",
            "      Successfully uninstalled charset-normalizer-2.1.1\n",
            "  Attempting uninstall: urllib3\n",
            "    Found existing installation: urllib3 1.24.3\n",
            "    Uninstalling urllib3-1.24.3:\n",
            "      Successfully uninstalled urllib3-1.24.3\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "aiohttp 3.8.3 requires charset-normalizer<3.0,>=2.0, but you have charset-normalizer 3.0.1 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed charset-normalizer-3.0.1 courlan-0.8.3 dateparser-1.1.6 htmldate-1.4.1 justext-3.0.0 tld-0.12.6 trafilatura-1.4.0 urllib3-1.26.14\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting feedgenerator\n",
            "  Downloading feedgenerator-2.0.0-py3-none-any.whl (21 kB)\n",
            "Requirement already satisfied: pytz>=0a in /usr/local/lib/python3.8/dist-packages (from feedgenerator) (2022.7)\n",
            "Installing collected packages: feedgenerator\n",
            "Successfully installed feedgenerator-2.0.0\n"
          ]
        }
      ],
      "source": [
        "!pip install feedparser \n",
        "\n",
        "!pip install trafilatura \n",
        "\n",
        "!pip install feedgenerator"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import trafilatura\n",
        "import feedparser\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "from feedgenerator import DefaultFeed\n"
      ],
      "metadata": {
        "id": "pFYK2s3bU5Gh"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Get text from a URL"
      ],
      "metadata": {
        "id": "y7Hk1L8j5Rsc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#download an example website\n",
        "downloaded = trafilatura.fetch_url('https://www.sciencedirect.com/science/article/abs/pii/S0048733320301402')"
      ],
      "metadata": {
        "id": "5DHlKDwr5GNi",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "de865a04-c8a6-46d5-9f71-25ea64d186f5"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "ERROR:trafilatura.downloads:not a 200 response: 403 for URL https://www.sciencedirect.com/science/article/abs/pii/S0048733320301402\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#extract the main text and show some of it\n",
        "\n",
        "trafilatura.extract(downloaded, include_comments=False, include_table=False)[:2000]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 139
        },
        "id": "ajKeqgSC57H-",
        "outputId": "5d78bb80-7fe4-4c51-90af-e4520bfe2bfb"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'No Truth Can Be Justified\\nBrett: Initial guesses at what ‘knowledge’ was all about amounted to what is known as the “justified true belief” vision of knowledge, and it’s still the most prevalent idea today. Anyone who calls themself a Bayesian is a justified true believer.\\nThis is the misconception that knowledge is about trying to justify your beliefs as true. And if you’ve done so, then you can say, “I know that thing.” If I can justify as true my theory of gravity, then I should believe that theory of gravity, and only then can I say that it’s known.\\nThe problem with this is that there is no method of showing any piece of knowledge is true. The improvement Deutsch promotes in his books is this vision that Popper gave us, that all we have are guesses about reality. They’re conjectures.\\nPeople think, “Oh, that sounds a bit wishy-washy. It’s just a guess.” Well, it’s not a random guess. It’s a guess that has stood up against trials, against attempts to show that it’s false. It’s not that everyone who decides to have a guess stands on equal footing.\\nWhen people are unable to show that something’s false—via this method of refutation—then we accept it as a piece of knowledge. This allows us to accept the fact that we’re going to be able to make progress in the future, because all of our knowledge is conjectural. All of it is our best guess at the time.\\nThere’s elasticity within the knowledge that allows us to say, “There’s going to be errors. We’re going to correct them and, thereby, be able to make progress into the infinite future.”\\nThis is unlike the previous conception of knowledge, which says, “Once you’ve justified something as true, well, it’s true.” If it’s true, that means there is nothing false about it and, therefore, it can’t possibly be refuted. That’s a very religious notion.\\nThe modern incantation of this is Bayesianism, which says, “You have a theory, you collect more evidence, and you become more and more confident over time that your theory is correct'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#create a function that does this\n",
        "\n",
        "def get_text(url):\n",
        "  downloaded = trafilatura.fetch_url(url )\n",
        "  return trafilatura.extract(downloaded, include_comments=False, include_tables=False)"
      ],
      "metadata": {
        "id": "o2OzAq3X6kAS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Summarization"
      ],
      "metadata": {
        "id": "q_b3fEws6_c6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Use BART-CNN model from Hugging Face using\n",
        "API_URL = \"https://api-inference.huggingface.co/models/facebook/bart-large-cnn\"\n",
        "\n",
        "# use your own read HF token\n",
        "headers = {\"Authorization\":\"Bearer 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\"}"
      ],
      "metadata": {
        "id": "KLbkuyP67DgR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#define a query function\n",
        "\n",
        "def query(payload):\n",
        "    response = requests.post(API_URL, headers=headers, json=payload)\n",
        "    return response.json()"
      ],
      "metadata": {
        "id": "0HGZf7tV--Ze"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#define a summarization function\n",
        "\n",
        "def summarize(text):\n",
        "  if text is None: return None\n",
        "  output = query({\n",
        "      \"inputs\": text[:2048],\n",
        "        \"max_length\":300,\n",
        "        \"min_length\":50, \n",
        "        \"do_sample\":False \n",
        "  })\n",
        "  return output\n"
      ],
      "metadata": {
        "id": "IcuugDt4Cnu6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#test the fuctions\n",
        "\n",
        "text = get_text('https://nav.al/justified')\n",
        "summarize(text)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QuEEl2St_4bg",
        "outputId": "6503010a-27cc-4dcc-950b-7446af968b80"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[{'summary_text': 'The “justified true belief” vision of knowledge is still the most prevalent idea today. Anyone who calls themself a Bayesian is a justified true believer. The improvement Deutsch promotes in his books is this vision that Popper gave us, that all we have are guesses about reality.'}]"
            ]
          },
          "metadata": {},
          "execution_count": 58
        }
      ]
    }
  ]
}